{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmrGqPg1NQoV"
      },
      "source": [
        "# Design Patterns with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Introduction to LangGraph\n",
        "\n",
        "**LangGraph** is a powerful framework built on top of **LangChain** that helps developers design **stateful, multi-step AI applications** using **graph-based workflows**.\n",
        "\n",
        "Instead of writing sequential chains or complex agents manually, LangGraph allows us to represent LLM logic as a **graph of nodes** ‚Äî where:\n",
        "- Each **node** represents a computation or decision step.\n",
        "- **Edges** define how data flows between nodes.\n",
        "- The entire graph maintains **state** throughout the conversation.\n",
        "\n",
        "Think of LangGraph as:\n",
        "> \"A framework that brings deterministic control to agentic workflows powered by LLMs.\"\n",
        "\n",
        "It blends the flexibility of traditional agents with the **reliability and observability** of a flow-based architecture.\n",
        "\n",
        "\n",
        "====================================================================================================================================================================================\n",
        "===================================================================================================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° LangGraph ‚Äì Theoretical Examples\n",
        "\n",
        "### üß≠ Example 1: Simple RAG Workflow\n",
        "A basic Retrieval-Augmented Generation (RAG) graph can have three nodes:\n",
        "\n",
        "1. **Query Analyzer Node** ‚Äì decides if the question needs retrieval.  \n",
        "2. **Retriever Node** ‚Äì fetches documents from the vector store.  \n",
        "3. **Answer Generator Node** ‚Äì uses the LLM to synthesize an answer.\n",
        "\n",
        "**Flow:**  \n",
        "User Query ‚ûú Query Analyzer ‚ûú Retriever ‚ûú Answer Generator ‚ûú Response\n",
        "\n",
        "This gives fine control: if a query doesn‚Äôt need retrieval, the graph can skip the retriever step.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Example 2: FAQ Chatbot with Guardrails\n",
        "For a PDF FAQ bot:\n",
        "- **Input Node:** takes user question.  \n",
        "- **Retriever Node:** fetches relevant chunks from the document.  \n",
        "- **Answer Node:** LLM generates a summary answer.  \n",
        "- **Verifier Node:** double-checks if the answer is grounded in source documents.  \n",
        "- **Fallback Node:** returns ‚ÄúI‚Äôm not sure‚Äù if confidence < threshold.\n",
        "\n",
        "This ensures reliability and factual accuracy.\n",
        "---\n",
        "\n",
        "‚úÖ **Key takeaway**\n",
        "LangGraph = *Visual + Deterministic + Stateful AI Flow Control.*\n",
        "\n",
        "You can visualize it as:\n",
        "\n",
        "====================================================================================================================================================================================\n",
        "===================================================================================================================================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi agent workflow with LangGraph, where a supervisor dynamically routes user queries to specialized agents like Weather, \n",
        "# Flights, and News. \n",
        "\n",
        "\n",
        "# Imports\n",
        "# Define LLM\n",
        "# Router Model\n",
        "\n",
        "# Agent\n",
        "# SuperVisor Agent\n",
        "# Weather Agent\n",
        "# News Agent\n",
        "\n",
        "# Graph Construction\n",
        "# Invoke Graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi agent workflow with LangGraph, where a supervisor dynamically routes user queries to specialized agents like Weather, \n",
        "# Flights, and News. \n",
        "# \n",
        "###############################################################\n",
        "# SETUP: IMPORTS & ENVIRONMENT\n",
        "###############################################################\n",
        "import operator\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.types import Command\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_success = load_dotenv()\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# LLM INITIALIZATION\n",
        "###############################################################\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        ")\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# UPDATED ROUTER MODEL ‚Äî ADD \"News\"\n",
        "###############################################################\n",
        "class Router(BaseModel):\n",
        "    \"\"\"\n",
        "    Decides the next agent to route to.\n",
        "    Now supports: Weather / Flights / News / __end__\n",
        "    \"\"\"\n",
        "    next_agent: Literal[\"Weather\", \"Flights\", \"News\", \"__end__\"]\n",
        "\n",
        "supervisor_model = llm.with_structured_output(Router)\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# SUPERVISOR FUNCTION\n",
        "###############################################################\n",
        "def supervisor(state: MessagesState) -> Command:\n",
        "    print(\"--- üßë‚Äçüíº SUPERVISOR ---\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a supervisor routing tasks to a specialist agent.\n",
        "    Based on the user's request, choose the appropriate agent.\n",
        "\n",
        "    Agents:\n",
        "    - Weather: For weather forecast questions.\n",
        "    - Flights: For flight information.\n",
        "    - News: For news or headlines.\n",
        "    \n",
        "    If the user is finishing the conversation, choose '__end__'.\n",
        "\n",
        "    User message: \"{state['messages'][-1].content}\"\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(state['messages'][-1], HumanMessage):\n",
        "        response = supervisor_model.invoke(prompt)\n",
        "        print(f\"Supervisor routing to: {response.next_agent}\")\n",
        "        return Command(goto=response.next_agent)\n",
        "    else:\n",
        "        return Command(goto=\"__end__\")\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# WEATHER AGENT\n",
        "###############################################################\n",
        "def weather_agent(state: MessagesState) -> Command:\n",
        "    print(\"--- ‚òÄÔ∏è WEATHER AGENT ---\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a weather forecaster.\n",
        "    Provide a short mock weather forecast for the user's location.\n",
        "\n",
        "    User message: \"{state['messages'][-1].content}\"\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    print(f\"Response: {response.content}\")\n",
        "\n",
        "    return Command(\n",
        "        goto=\"supervisor\",\n",
        "        update={\"messages\": [response]},\n",
        "    )\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# FLIGHTS AGENT\n",
        "###############################################################\n",
        "def flights_agent(state: MessagesState) -> Command:\n",
        "    print(\"--- ‚úàÔ∏è FLIGHTS AGENT ---\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a flight information assistant.\n",
        "    Give a short mock flight detail for user's requested destination.\n",
        "\n",
        "    User message: \"{state['messages'][-1].content}\"\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    print(f\"Response: {response.content}\")\n",
        "\n",
        "    return Command(\n",
        "        goto=\"supervisor\",\n",
        "        update={\"messages\": [response]},\n",
        "    )\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# ‚≠ê NEW NEWS AGENT\n",
        "###############################################################\n",
        "def news_agent(state: MessagesState) -> Command:\n",
        "    print(\"--- üì∞ NEWS AGENT ---\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a news assistant.\n",
        "    Provide 2‚Äì3 mock breaking news headlines relevant to the user's message.\n",
        "\n",
        "    User message: \"{state['messages'][-1].content}\"\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    print(f\"Response: {response.content}\")\n",
        "\n",
        "    return Command(\n",
        "        goto=\"supervisor\",\n",
        "        update={\"messages\": [response]},\n",
        "    )\n",
        "\n",
        "\n",
        "###############################################################\n",
        "# GRAPH CONSTRUCTION\n",
        "###############################################################\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "builder.add_node(\"supervisor\", supervisor)\n",
        "builder.add_node(\"Weather\", weather_agent)\n",
        "builder.add_node(\"Flights\", flights_agent)\n",
        "builder.add_node(\"News\", news_agent)  # <-- NEW NODE REGISTERED\n",
        "\n",
        "builder.add_edge(START, \"supervisor\")\n",
        "\n",
        "graph = builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='What will be the weather like in Chennai tomorrow morning?', additional_kwargs={}, response_metadata={}, id='e9960eb4-b5b0-46e0-ab9d-bbc30c1898d1')]\n",
            "----\n",
            "Supervisor routing to: Weather\n",
            "--- ‚òÄÔ∏è WEATHER AGENT ---\n",
            "Response: Good morning! Here's your forecast for Chennai tomorrow morning:\n",
            "\n",
            "Expect partly cloudy skies with a chance of light drizzle. Temperatures will be around 28 degrees Celsius. The humidity will be high, making it feel a little warmer. Winds will be light and variable. Have a great morning!\n",
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='What will be the weather like in Chennai tomorrow morning?', additional_kwargs={}, response_metadata={}, id='e9960eb4-b5b0-46e0-ab9d-bbc30c1898d1'), AIMessage(content=\"Good morning! Here's your forecast for Chennai tomorrow morning:\\n\\nExpect partly cloudy skies with a chance of light drizzle. Temperatures will be around 28 degrees Celsius. The humidity will be high, making it feel a little warmer. Winds will be light and variable. Have a great morning!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--967eac62-c070-4a63-9ca2-8892a22a3cb3-0', usage_metadata={'input_tokens': 43, 'output_tokens': 61, 'total_tokens': 104, 'input_token_details': {'cache_read': 0}})]\n",
            "----\n",
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='Is it going to rain in Bangalore this weekend?', additional_kwargs={}, response_metadata={}, id='781b9bb1-e887-4f5d-a5cf-c97d16b9647e')]\n",
            "----\n",
            "Supervisor routing to: Weather\n",
            "--- ‚òÄÔ∏è WEATHER AGENT ---\n",
            "Response: Okay, here's your weekend weather forecast for Bangalore:\n",
            "\n",
            "Good news! It looks like Bangalore is going to have a pleasant weekend. We're expecting mostly sunny skies on both Saturday and Sunday. Temperatures will be mild, with highs around 28 degrees Celsius (82 degrees Fahrenheit) and lows around 19 degrees Celsius (66 degrees Fahrenheit). There's a very low chance of rain, so you can leave your umbrellas at home. Enjoy the sunshine!\n",
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='Is it going to rain in Bangalore this weekend?', additional_kwargs={}, response_metadata={}, id='781b9bb1-e887-4f5d-a5cf-c97d16b9647e'), AIMessage(content=\"Okay, here's your weekend weather forecast for Bangalore:\\n\\nGood news! It looks like Bangalore is going to have a pleasant weekend. We're expecting mostly sunny skies on both Saturday and Sunday. Temperatures will be mild, with highs around 28 degrees Celsius (82 degrees Fahrenheit) and lows around 19 degrees Celsius (66 degrees Fahrenheit). There's a very low chance of rain, so you can leave your umbrellas at home. Enjoy the sunshine!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--bc386110-fe1d-47f7-b040-5e082ddae1ef-0', usage_metadata={'input_tokens': 42, 'output_tokens': 98, 'total_tokens': 140, 'input_token_details': {'cache_read': 0}})]\n",
            "----\n",
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='What are today‚Äôs top world news headlines?', additional_kwargs={}, response_metadata={}, id='38ebfef2-9446-4f69-bafb-aa2327d77b70')]\n",
            "----\n",
            "Supervisor routing to: News\n",
            "--- üì∞ NEWS AGENT ---\n",
            "Response: Okay, here are a few mock breaking news headlines based on your request for top world news:\n",
            "\n",
            "*   **Ukraine Conflict Escalates: New Offensive Reported in Eastern Region**\n",
            "*   **Global Inflation Concerns Rise as Major Economies Announce Higher Than Expected Rates**\n",
            "*   **International Tensions Flare Over Disputed Territory in South China Sea**\n",
            "--- üßë‚Äçüíº SUPERVISOR ---\n",
            "----\n",
            "[HumanMessage(content='What are today‚Äôs top world news headlines?', additional_kwargs={}, response_metadata={}, id='38ebfef2-9446-4f69-bafb-aa2327d77b70'), AIMessage(content='Okay, here are a few mock breaking news headlines based on your request for top world news:\\n\\n*   **Ukraine Conflict Escalates: New Offensive Reported in Eastern Region**\\n*   **Global Inflation Concerns Rise as Major Economies Announce Higher Than Expected Rates**\\n*   **International Tensions Flare Over Disputed Territory in South China Sea**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d416cc7e-8031-4662-ab84-100551fc86f0-0', usage_metadata={'input_tokens': 45, 'output_tokens': 71, 'total_tokens': 116, 'input_token_details': {'cache_read': 0}})]\n",
            "----\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # The graph.stream() method invokes the graph and streams the results.\n",
        "#     What is the weather in Chennai tomorrow?\n",
        "    # Find me flights to Bangalore\n",
        "    events = graph.stream({\"messages\":[HumanMessage(content=user_input)]})\n",
        "    for event in events:\n",
        "        # We only print the AI's responses to the user.\n",
        "        if \"messages\" in event:\n",
        "            event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi agent workflow with LangGraph, where a supervisor dynamically routes user queries to specialized agents like Weather, \n",
        "# Flights, and News. \n",
        "\n",
        "\n",
        "# Imports\n",
        "# Define LLM\n",
        "# Router Model\n",
        "\n",
        "# generator\n",
        "# evaluator\n",
        "# decide\n",
        "\n",
        "# Graph Construction\n",
        "# Invoke Graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Workflow with three nodes - generator, evaluator, and finalize\n",
        "# This iteratively improves an LLM-generated draft based on feedback\n",
        "# It loops between generation and evaluation until the evaluator returns ‚ÄúAPPROVED,‚Äù then outputs the final draft.\n",
        "\n",
        "import operator\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.types import Command\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_success = load_dotenv()\n",
        "\n",
        "# LLM client\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        ")\n",
        "\n",
        "# State\n",
        "class State(TypedDict):\n",
        "    task: str\n",
        "    draft: str\n",
        "    feedback: str\n",
        "    final: str\n",
        "\n",
        "# Nodes\n",
        "def generator(state: State):\n",
        "    \"\"\"Generate an initial or refined draft.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant helping to complete the following task:\n",
        "\n",
        "Task:\n",
        "{state['task']}\n",
        "\n",
        "Current Draft:\n",
        "{state.get('draft', 'None')}\n",
        "\n",
        "Feedback:\n",
        "{state.get('feedback', 'None')}\n",
        "\n",
        "Instructions:\n",
        "- If there is no draft and no feedback, generate a clear and complete response to the task.\n",
        "- If there is a draft but no feedback, improve the draft as needed for clarity and quality.\n",
        "- If there is both a draft and feedback, revise the draft by incorporating the feedback directly.\n",
        "- Always produce a single, improved draft as your output.\n",
        "\"\"\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    return {\"draft\": resp.content.strip()}\n",
        "\n",
        "def evaluator(state: State):\n",
        "    \"\"\"Evaluate the draft and give feedback or approval.\"\"\"\n",
        "    prompt = f\"\"\"Evaluate the following draft, based on the given task.\n",
        "If it meets the requirements, reply exactly 'APPROVED'.\n",
        "Otherwise, provide constructive feedback for improvement.\n",
        "Task:\n",
        "{state['task']}\n",
        "Draft:\n",
        "{state['draft']}\"\"\"\n",
        "    resp = llm.invoke(prompt)\n",
        "    print(f\"\"\"\n",
        "================= DRAFT =================\n",
        "{state['draft']}\n",
        "\n",
        "================ FEEDBACK ===============\n",
        "{resp.content.strip()}\n",
        "========================================\n",
        "\"\"\")\n",
        "    return {\"feedback\": resp.content.strip()}\n",
        "\n",
        "def decide(state: State) -> str:\n",
        "    \"\"\"Decide next step: either approve and finish, or refine again.\"\"\"\n",
        "    if \"APPROVED\" in state[\"feedback\"].upper():\n",
        "        return \"approved\"\n",
        "    return \"refine\"\n",
        "\n",
        "def finalize(state: State):\n",
        "    \"\"\"Save the final approved draft.\"\"\"\n",
        "    return {\"final\": state[\"draft\"]}\n",
        "\n",
        "# Build the graph\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"generator\", generator)\n",
        "builder.add_node(\"evaluator\", evaluator)\n",
        "builder.add_node(\"finalize\", finalize)\n",
        "\n",
        "builder.add_edge(START, \"generator\")\n",
        "builder.add_edge(\"generator\", \"evaluator\")\n",
        "builder.add_edge(\"evaluator\", \"finalize\")\n",
        "\n",
        "# Conditional edges from decide\n",
        "builder.add_conditional_edges(\n",
        "    \"evaluator\",\n",
        "    decide,\n",
        "    {\n",
        "        \"approved\": \"finalize\",   # stop loop\n",
        "        \"refine\": \"generator\",    # go back for improvement\n",
        "    },\n",
        ")\n",
        "\n",
        "builder.add_edge(\"finalize\", END)\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================= DRAFT =================\n",
            "Scaling a Node.js application to handle 100,000 concurrent users presents several key challenges. These can be broadly categorized into infrastructure, application code, database, and monitoring/operations.\n",
            "\n",
            "**1. Infrastructure Limitations:**\n",
            "\n",
            "*   **Single-Threaded Nature of Node.js:** Node.js, while event-driven and non-blocking, primarily operates on a single thread. This can become a bottleneck when handling CPU-intensive tasks. Techniques like clustering (using Node.js's `cluster` module or process managers like PM2) or offloading CPU-intensive tasks to worker threads are crucial.\n",
            "*   **Server Capacity:** A single server likely won't be sufficient. Load balancing across multiple servers is essential to distribute the load and prevent overload. This requires choosing a suitable load balancer (e.g., Nginx, HAProxy, cloud provider load balancers).\n",
            "*   **Network Bandwidth:** Sufficient network bandwidth is critical to handle the increased traffic. This includes bandwidth for both incoming requests and outgoing responses.\n",
            "*   **Resource Limits:** Each server has finite resources (CPU, memory, I/O). Careful resource allocation and monitoring are required to ensure optimal performance and prevent resource exhaustion.\n",
            "\n",
            "**2. Application Code Challenges:**\n",
            "\n",
            "*   **Blocking Operations:** Blocking operations (e.g., synchronous file I/O, long-running calculations, inefficient database queries) will severely impact performance. Identifying and replacing these with asynchronous alternatives is crucial.\n",
            "*   **Memory Leaks:** Memory leaks can gradually degrade performance and eventually crash the application. Thorough code reviews, profiling tools, and proper memory management practices are essential.\n",
            "*   **Inefficient Code:** Poorly optimized code can consume excessive CPU and memory. Code profiling and optimization are necessary to improve performance.\n",
            "*   **Session Management:** Managing sessions for a large number of concurrent users can be challenging. In-memory session storage becomes impractical. Solutions like Redis or Memcached for distributed session management are required.\n",
            "*   **WebSockets:** If using WebSockets, managing a large number of persistent connections efficiently requires careful consideration of connection limits and resource usage. Techniques like horizontal scaling of WebSocket servers and efficient message handling are needed.\n",
            "\n",
            "**3. Database Bottlenecks:**\n",
            "\n",
            "*   **Database Performance:** The database is often a major bottleneck. Optimizing database queries, using indexes effectively, and considering database sharding or replication are crucial.\n",
            "*   **Connection Pooling:** Managing a large number of database connections can be resource-intensive. Connection pooling helps reuse existing connections and reduce the overhead of establishing new connections.\n",
            "*   **Caching:** Implementing caching mechanisms (e.g., using Redis or Memcached) can significantly reduce database load by storing frequently accessed data in memory.\n",
            "*   **Database Choice:** The choice of database (SQL vs. NoSQL) can impact scalability. Consider the specific needs of the application and choose a database that can handle the expected load and data volume.\n",
            "\n",
            "**4. Monitoring and Operations:**\n",
            "\n",
            "*   **Monitoring:** Comprehensive monitoring of server resources, application performance, and database activity is essential to identify bottlenecks and potential issues. Tools like Prometheus, Grafana, and ELK stack are helpful.\n",
            "*   **Logging:** Effective logging is crucial for debugging and troubleshooting issues. Implement structured logging and ensure that logs are easily accessible and searchable.\n",
            "*   **Deployment and Automation:** Automated deployment processes (e.g., using Docker and Kubernetes) are essential for quickly deploying updates and scaling the application.\n",
            "*   **Error Handling:** Robust error handling is crucial to prevent application crashes and ensure a smooth user experience. Implement proper error logging, monitoring, and alerting.\n",
            "*   **Security:** Security considerations are paramount when handling a large number of users. Implement appropriate security measures to protect against attacks and data breaches. This includes input validation, authentication, authorization, and regular security audits.\n",
            "\n",
            "================ FEEDBACK ===============\n",
            "APPROVED\n",
            "========================================\n",
            "\n",
            "\n",
            "Final Answer:\n",
            " Scaling a Node.js application to handle 100,000 concurrent users presents several key challenges. These can be broadly categorized into infrastructure, application code, database, and monitoring/operations.\n",
            "\n",
            "**1. Infrastructure Limitations:**\n",
            "\n",
            "*   **Single-Threaded Nature of Node.js:** Node.js, while event-driven and non-blocking, primarily operates on a single thread. This can become a bottleneck when handling CPU-intensive tasks. Techniques like clustering (using Node.js's `cluster` module or process managers like PM2) or offloading CPU-intensive tasks to worker threads are crucial.\n",
            "*   **Server Capacity:** A single server likely won't be sufficient. Load balancing across multiple servers is essential to distribute the load and prevent overload. This requires choosing a suitable load balancer (e.g., Nginx, HAProxy, cloud provider load balancers).\n",
            "*   **Network Bandwidth:** Sufficient network bandwidth is critical to handle the increased traffic. This includes bandwidth for both incoming requests and outgoing responses.\n",
            "*   **Resource Limits:** Each server has finite resources (CPU, memory, I/O). Careful resource allocation and monitoring are required to ensure optimal performance and prevent resource exhaustion.\n",
            "\n",
            "**2. Application Code Challenges:**\n",
            "\n",
            "*   **Blocking Operations:** Blocking operations (e.g., synchronous file I/O, long-running calculations, inefficient database queries) will severely impact performance. Identifying and replacing these with asynchronous alternatives is crucial.\n",
            "*   **Memory Leaks:** Memory leaks can gradually degrade performance and eventually crash the application. Thorough code reviews, profiling tools, and proper memory management practices are essential.\n",
            "*   **Inefficient Code:** Poorly optimized code can consume excessive CPU and memory. Code profiling and optimization are necessary to improve performance.\n",
            "*   **Session Management:** Managing sessions for a large number of concurrent users can be challenging. In-memory session storage becomes impractical. Solutions like Redis or Memcached for distributed session management are required.\n",
            "*   **WebSockets:** If using WebSockets, managing a large number of persistent connections efficiently requires careful consideration of connection limits and resource usage. Techniques like horizontal scaling of WebSocket servers and efficient message handling are needed.\n",
            "\n",
            "**3. Database Bottlenecks:**\n",
            "\n",
            "*   **Database Performance:** The database is often a major bottleneck. Optimizing database queries, using indexes effectively, and considering database sharding or replication are crucial.\n",
            "*   **Connection Pooling:** Managing a large number of database connections can be resource-intensive. Connection pooling helps reuse existing connections and reduce the overhead of establishing new connections.\n",
            "*   **Caching:** Implementing caching mechanisms (e.g., using Redis or Memcached) can significantly reduce database load by storing frequently accessed data in memory.\n",
            "*   **Database Choice:** The choice of database (SQL vs. NoSQL) can impact scalability. Consider the specific needs of the application and choose a database that can handle the expected load and data volume.\n",
            "\n",
            "**4. Monitoring and Operations:**\n",
            "\n",
            "*   **Monitoring:** Comprehensive monitoring of server resources, application performance, and database activity is essential to identify bottlenecks and potential issues. Tools like Prometheus, Grafana, and ELK stack are helpful.\n",
            "*   **Logging:** Effective logging is crucial for debugging and troubleshooting issues. Implement structured logging and ensure that logs are easily accessible and searchable.\n",
            "*   **Deployment and Automation:** Automated deployment processes (e.g., using Docker and Kubernetes) are essential for quickly deploying updates and scaling the application.\n",
            "*   **Error Handling:** Robust error handling is crucial to prevent application crashes and ensure a smooth user experience. Implement proper error logging, monitoring, and alerting.\n",
            "*   **Security:** Security considerations are paramount when handling a large number of users. Implement appropriate security measures to protect against attacks and data breaches. This includes input validation, authentication, authorization, and regular security audits.\n"
          ]
        }
      ],
      "source": [
        "# Run example\n",
        "# You have six horses and want to race them to see which is fastest. What is the best way to do this?\n",
        "# Write a clear and concise introduction for a blog post explaining what LangGraph is and why developers should use it.\n",
        "# Summarize the key challenges in scaling a Node.js application to support 100k concurrent users.\n",
        "\n",
        "input_task = \"Summarize the key challenges in scaling a Node.js application to support 100k concurrent users.\"\n",
        "result = graph.invoke({\"task\": input_task})\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\", result[\"final\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".genai-session",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
