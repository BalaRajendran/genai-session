{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langfuse import observe\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_success = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ee0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Translated Text ---\n",
      "Bonjour, qu'est-ce qui se passe ?\n"
     ]
    }
   ],
   "source": [
    "# Langchain with simple example\n",
    "# Then we will try types of prompting\n",
    "    # 1. Zero Shot Prompt\n",
    "    # 2. One Shot \n",
    "    # 3. Few Shot Prompt \n",
    "    # 4. Chain of Though\n",
    "# RAG with PDF extraction\n",
    "\n",
    "# Simple translation pipeline using LangChain and OpenAI\n",
    "\n",
    "# Language\n",
    "# Input Text\n",
    "\n",
    "# Transalated Text\n",
    "\n",
    "# 1. Create LLM Object\n",
    "# 2. Build a Prompt Template\n",
    "# 3. Compose the prompt and model\n",
    "# 4. Execute the chain\n",
    "\n",
    "# 1. Create an LLM client object and we are going to use gpt 4 model\n",
    "#    - 'ChatOpenAI' is a LangChain wrapper around a chat model.\n",
    "#    - model=\"gpt-4o-mini\" selects which model to talk to.\n",
    "#    - You can later pass settings like temperature, max_tokens, streaming, etc.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 2. Prompting is a we can communicate with LLM\n",
    "# 2. Build a PromptTemplate.\n",
    "#    - input_variables defines placeholders we will fill later.\n",
    "#    - template is the string with placeholders: {language} and {text}.\n",
    "#    - The template ensures we send a consistent instruction to the model.\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"language\"],\n",
    "    template=\"Translate the following sentence into {language}:\\n\\n{text}\",\n",
    ")\n",
    "\n",
    "# 3. Compose the prompt and model into a chain using the pipe operator.\n",
    "#    - This expression wires the template output into the LLM as input.\n",
    "#    - In other words: when executed, the filled-in prompt is sent to `llm`.\n",
    "#    - Many LangChain versions allow \"piping\" components like this for a simple flow.\n",
    "chain = prompt | llm\n",
    "\n",
    "# 4. Execute the chain.\n",
    "#    - invoke takes a dict mapping input variable names to values.\n",
    "#    - It runs the prompt template (fills placeholders), sends it to the LLM and returns a response object.\n",
    "\n",
    "text = \"Good morning, Whats going on??\"\n",
    "language = \"French\"\n",
    "\n",
    "result = chain.invoke({\"text\": text, \"language\": language})\n",
    "\n",
    "print(\"--- Translated Text ---\")\n",
    "print(result.content)\n",
    "\n",
    "\n",
    "# The model performs a task without seeing any examples.\n",
    "# You just give an instruction â€” the model uses its internal knowledge to answer.\n",
    "\n",
    "# Example\n",
    "# Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "# Do not generate any additional texts or explainations, return only the sentiment.\n",
    "# Your output should be one of the following:[\"positive\",negative\"]\n",
    "# Text: {query}\n",
    "# Sentiment:\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# One-Shot Prompting\n",
    "# You provide one example of how the task should be done before asking for the actual output.\n",
    "\n",
    "# Example\n",
    "\n",
    "# Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "# Do not generate any additional texts or explainations, return only the sentiment.\n",
    "# Your output should be one of the following:[\"positive\",negative\"]\n",
    "# Example 01\n",
    "# Text: The film was good but hate the villain, I hate vaccations, The film was good, but i dont like the climax, \n",
    "# Sentiment: Positive\n",
    "\n",
    "# Text:{query} Sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d2c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Translated Text ---\n",
      "To determine the duration of the train journey, we can follow these steps:\n",
      "\n",
      "1. **Identify the departure time**: The train leaves at 3 PM.\n",
      "2. **Identify the arrival time**: The train arrives at 6 PM.\n",
      "3. **Calculate the duration**: To find the duration, we subtract the departure time from the arrival time.\n",
      "\n",
      "   - From 3 PM to 4 PM is 1 hour.\n",
      "   - From 4 PM to 5 PM is another hour (totaling 2 hours).\n",
      "   - From 5 PM to 6 PM is yet another hour (totaling 3 hours).\n",
      "\n",
      "So, the total journey time is **3 hours**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The model performs a task without seeing any examples.\n",
    "# You just give an instruction â€” the model uses its internal knowledge to answer.\n",
    "\n",
    "# Example\n",
    "# Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "# Do not generate any additional texts or explainations, return only the sentiment.\n",
    "# Your output should be one of the following:[\"positive\",negative\"]\n",
    "# Text: {query}\n",
    "# Sentiment:\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# One-Shot Prompting\n",
    "# You provide one example of how the task should be done before asking for the actual output.\n",
    "\n",
    "# Example\n",
    "\n",
    "# Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "# Do not generate any additional texts or explainations, return only the sentiment.\n",
    "# Your output should be one of the following:[\"positive\",negative\"]\n",
    "# Example 01\n",
    "# Text: The film was good but hate the villain, I hate vaccations, The film was good, but i dont like the climax, \n",
    "# Sentiment: Positive\n",
    "\n",
    "# Text:{query} Sentiment:\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Few-Shot Prompting\n",
    "# You provide one example of how the task should be done before asking for the actual output.\n",
    "\n",
    "# Example\n",
    "\n",
    "# Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "# Do not generate any additional texts or explainations, return only the sentiment.\n",
    "# Your output should be one of the following:[\"positive\",negative\"]\n",
    "# Example 01\n",
    "# Text: The film was good but hate the villain # I hate vaccations, The film was good, but i dont like the climax, \n",
    "# Sentiment: Positive\n",
    "# Example 02\n",
    "# Text: I hate vaccations\n",
    "# Sentiment: Positive\n",
    "\n",
    "# Text:{query} Sentiment:\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Chain of Thought Prompting\n",
    "# You ask the model to â€œthink step-by-stepâ€ before answering. Helps with reasoning, math, and logical tasks.\n",
    "\n",
    "# Example\n",
    "# If a train leaves at 3PM and arrives at 6PM, how long was the journey?\n",
    "# Let's think step by step.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "input_prompt=\"\"\"\n",
    "If a train leaves at 3PM and arrives at 6PM, how long was the journey?\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "query = \"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=input_prompt,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Translated Text ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79531abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Believe in the power of your dreams; they are the blueprints of your future.\"\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize LangFuse Client\n",
    "# 2. Define Observable Function\n",
    "# 3. Initialize LLM\n",
    "# 4. Create Prompt Template\n",
    "# 5. Create and Run Chain\n",
    "# 6. Return output content\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ STEP 1: Initialize LangFuse Client\n",
    "# ---------------------------------------------\n",
    "langfuse_handler = CallbackHandler()  # Creates callback for tracing\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ STEP 2: Define Observable Function\n",
    "# ---------------------------------------------\n",
    "@observe(name=\"motivational_quote_flow\")\n",
    "def get_quote(language: str):\n",
    "    \"\"\"Generate a motivational quote in the given language.\"\"\"\n",
    "\n",
    "    # ðŸ”¹ STEP 3: Initialize LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", callbacks=[langfuse_handler])\n",
    "\n",
    "    # ðŸ”¹ STEP 4: Create Prompt Template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"language\"],\n",
    "        template=\"Write a short motivational quote in {language}.\",\n",
    "    )\n",
    "\n",
    "    # ðŸ”¹ STEP 5: Create and Run Chain\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"language\": language})\n",
    "\n",
    "    # ðŸ”¹ STEP 6: Return output content\n",
    "    return result.content\n",
    "\n",
    "\n",
    "# ðŸ”¹ STEP 7: Test Function\n",
    "print(get_quote(\"English\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff40e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation='25 * 12 + 36' result='336' explanation='First, multiply 25 by 12, which equals 300. Then, add 36 to 300, resulting in 336.'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ Step 1: Define Output Schema\n",
    "# ---------------------------------------------\n",
    "class ResponseSchema(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "\n",
    "    calculation: str = Field(description=\"The mathematical expression\")\n",
    "    result: str = Field(description=\"The numerical answer\")\n",
    "    explanation: str = Field(description=\"Step-by-step explanation\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ Step 2: Define a Tool\n",
    "# ---------------------------------------------\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluates a math expression and returns the result.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ Step 3: Initialize LLM and Agent\n",
    "# ---------------------------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[calculator],\n",
    "    debug=False,\n",
    "    response_format=ResponseSchema,\n",
    "    system_prompt=\"You are a Mathematical assistant.\",\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ðŸ”¹ Step 4: Ask a question\n",
    "# ---------------------------------------------\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is 25 * 12 + 36?\"}]}\n",
    ")\n",
    "\n",
    "print(result[\"structured_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a29b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# 1. Load PDF\n",
    "pdf_path = \"./pdf/pondicherry.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create embeddings & store in Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebeeb7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Question: What are the main attractions in Pondicherry?\n",
      "--------------------------------------------------\n",
      "Answer: The main attractions in Pondicherry include the Sri Aurobindo Ashram, various old churches from the 18th and 19th centuries, and heritage buildings around Promenade Beach such as the Gandhi statue, French War Memorial, and the 19th Century Lighthouse. Other notable sites are the Puducherry Botanical Gardens, Chunnambar Backwater resort, and the Arulmigu Manakula Vinayagar Devasthanam temple. Additionally, visitors can explore the Romain Rolland Library, Pondicherry Museum, and the French Institute of Pondicherry.\n",
      "--------------------------------------------------\n",
      "Source documents used: 518 chunks\n"
     ]
    }
   ],
   "source": [
    "# Retrieval and Generation\n",
    "question = \"What are the main attractions in Pondicherry?\"\n",
    "\n",
    "# 1. Load Chroma DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 2. Retrieve relevant chunks\n",
    "retrieved_docs = vector_db.similarity_search(question ,k=2)\n",
    "\n",
    "# 3. Combine retrieved text into context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# 4. Create the system + user message\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# 5. Create ChatOpenAI and call it\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 6. Build the message sequence manually\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Answer: {response.content}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source documents used: {len(response.content)} chunks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag-session (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
