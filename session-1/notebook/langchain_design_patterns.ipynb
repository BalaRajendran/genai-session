{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6377222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_success = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39467613",
   "metadata": {},
   "source": [
    "# Langchain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283ee0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Translated Text ---\n",
      "Bonjour, que se passe-t-il ?\n"
     ]
    }
   ],
   "source": [
    "# Simple translation pipeline using LangChain and OpenAI\n",
    "\n",
    "# Language\n",
    "# Input Text\n",
    "\n",
    "# Transalated Text\n",
    "\n",
    "# 1. Create LLM Object\n",
    "# 2. Build a Prompt Template\n",
    "# 3. Compose the prompt and model\n",
    "# 4. Execute the chain\n",
    "\n",
    "# 1. Create an LLM client object and we are going to use gpt 4 model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 2. Build a PromptTemplate.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"language\"],\n",
    "    template=\"Translate the following sentence into {language}:\\n\\n{text}\",\n",
    ")\n",
    "\n",
    "# 3. Compose the prompt and model into a chain using the pipe operator.\n",
    "chain = prompt | llm\n",
    "\n",
    "# 4. Execute the chain.\n",
    "text = \"Good morning, Whats going on??\"\n",
    "language = \"French\"\n",
    "\n",
    "result = chain.invoke({\"text\": text, \"language\": language})\n",
    "\n",
    "print(\"--- Translated Text ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a1cb4",
   "metadata": {},
   "source": [
    "# Types of Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c29544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Output ---\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot Prompting\n",
    "# The model performs a task without seeing any examples.\n",
    "# You just give an instruction — the model uses its internal knowledge to answer.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "input_prompt=\"\"\"\n",
    "Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "Do not generate any additional texts or explainations, return only the sentiment.\n",
    "Your output should be one of the following:[\"positive\",negative\"]\n",
    "Text: {query}\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "query = \"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=input_prompt,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Output ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c514ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Output ---\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# One-Shot Prompting\n",
    "# You provide one example of how the task should be done before asking for the actual output.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "input_prompt=\"\"\"\n",
    "Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "Do not generate any additional texts or explainations, return only the sentiment.\n",
    "Your output should be one of the following:[\"positive\", \"negative\"]\n",
    "\n",
    "Example 01\n",
    "Text: The film was good but hate the villain, I hate vaccations, The film was good, but i dont like the climax, \n",
    "Sentiment: Positive\n",
    "\n",
    "Text:{query} Sentiment:\n",
    "\"\"\"\n",
    "query = \"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=input_prompt,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Output ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207218a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Output ---\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Prompting\n",
    "# You provide one example of how the task should be done before asking for the actual output.\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "input_prompt=\"\"\"\n",
    "Classify the sentiment of the following text as 'positive' or 'negative'.\n",
    "Do not generate any additional texts or explainations, return only the sentiment.\n",
    "Your output should be one of the following:[\"positive\",negative\"]\n",
    "Example 01\n",
    "Text: The film was good but hate the villain # I hate vaccations, The film was good, but i dont like the climax, \n",
    "Sentiment: Positive\n",
    "Example 02\n",
    "Text: I hate vaccations\n",
    "Sentiment: Positive\n",
    "\n",
    "Text:{query} Sentiment:\n",
    "\"\"\"\n",
    "query = \"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=input_prompt,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Output ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5976e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Output ---\n",
      "To determine the duration of the train journey, we can follow these steps:\n",
      "\n",
      "1. **Identify the departure time**: The train leaves at 3 PM.\n",
      "2. **Identify the arrival time**: The train arrives at 6 PM.\n",
      "3. **Calculate the time difference**: \n",
      "\n",
      "   - From 3 PM to 4 PM is 1 hour.\n",
      "   - From 4 PM to 5 PM is another hour (totaling 2 hours).\n",
      "   - From 5 PM to 6 PM is one more hour (totaling 3 hours).\n",
      "\n",
      "So, the total journey time is 3 hours.\n",
      "\n",
      "Therefore, the journey lasted **3 hours**.\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought Prompting\n",
    "# You ask the model to “think step-by-step” before answering. Helps with reasoning, math, and logical tasks.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "input_prompt=\"\"\"\n",
    "If a train leaves at 3PM and arrives at 6PM, how long was the journey?\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "query = \"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=input_prompt,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"--- Output ---\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d85492",
   "metadata": {},
   "source": [
    "# RAG Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79a29b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# 1. Load PDF\n",
    "pdf_path = \"./pdf/pondicherry.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create embeddings & store in Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebeeb7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Question: What are the main attractions in Pondicherry?\n",
      "--------------------------------------------------\n",
      "Answer: The main attractions in Pondicherry include the old churches built in the 18th and 19th centuries, heritage buildings and monuments around Promenade Beach, and the Puducherry Botanical Gardens. Notable sites also include the Sri Manakula Vinayagar Temple, French War Memorial, and the Pondicherry Museum. Additionally, the Chunnambar Backwater resort offers a tropical experience along with various parks and statues.\n",
      "--------------------------------------------------\n",
      "Source documents used: 417 chunks\n"
     ]
    }
   ],
   "source": [
    "# Retrieval and Generation\n",
    "question = \"What are the main attractions in Pondicherry?\"\n",
    "\n",
    "# 1. Load Chroma DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 2. Retrieve relevant chunks\n",
    "retrieved_docs = vector_db.similarity_search(question ,k=2)\n",
    "\n",
    "# 3. Combine retrieved text into context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# 4. Create the system + user message\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# 5. Create ChatOpenAI and call it\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 6. Build the message sequence manually\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Answer: {response.content}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Source documents used: {len(response.content)} chunks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag-session (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
